\documentclass[6pt, DIV=12]{scrartcl}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage[numbers]{natbib}

\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{url}
\usepackage{todonotes}
\usepackage{multirow}
\usepackage{tikz}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{thmtools}		
\usepackage{mleftright}
\usepackage{stmaryrd}
\usepackage{nicefrac}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{remark}[theorem]{Remark}

\usepackage{thm-restate}
\usepackage[mathic=true]{mathtools}
\usepackage{fixmath}
\usepackage{siunitx}
\usepackage{color}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{enumitem}
\setlist[enumerate]{noitemsep, topsep=0.5\topsep}
\setlist[description]{noitemsep, topsep=0.5\topsep}
\setlist[itemize]{noitemsep, topsep=0.5\topsep}

\usepackage{setspace}
\usepackage[protrusion=true,expansion=false, activate={true,nocompatibility},final,kerning=true,spacing=true]{microtype}
\usepackage[]{microtype}
\usepackage{ellipsis}
\usepackage{xspace}
\usepackage{hfoldsty}

\usepackage{tcolorbox}
\usepackage{lmodern}

%\addtokomafont{disposition}{\rmfamily}
%\addtokomafont{descriptionlabel}{\rmfamily}

%\usepackage{newpxtext,newpxmath}
%\usepackage{classico}

%\usepackage[tt=false]{libertine}
%\usepackage[varqu]{zi4}
%\usepackage[libertine]{newtxmath}

\usepackage[
pdfa,
hidelinks,
pdftex, 
pdfdisplaydoctitle,
pdfpagelabels,
pdfauthor={},
pdftitle={},
pdfsubject={},
pdfkeywords={},
pdfproducer={Latex with the hyperref package},
pdfcreator={pdflatex}
]{hyperref}

% Let cleveref and thmtools work together
\makeatletter
\def\thmt@refnamewithcomma #1#2#3,#4,#5\@nil{%
	\@xa\def\csname\thmt@envname #1utorefname\endcsname{#3}%
	\ifcsname #2refname\endcsname
	\csname #2refname\expandafter\endcsname\expandafter{\thmt@envname}{#3}{#4}%
	\fi
}
\makeatother
\usepackage[capitalise,noabbrev]{cleveref}   
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\algdef{SE}{ParForAllLoop}{EndParForAllLoop}[1]{\textbf{parallel for}\(\mbox{#1}\) \textbf{do}}{\textbf{end}}

\newcommand{\new}[1]{\emph{#1}}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\newcommand{\cA}{\ensuremath{{\mathcal A}}\xspace}
\newcommand{\cB}{\ensuremath{{\mathcal B}}\xspace}
\newcommand{\cC}{\ensuremath{{\mathcal C}}\xspace}
\newcommand{\cD}{\ensuremath{{\mathcal D}}\xspace}
\newcommand{\cF}{\ensuremath{{\mathcal F}}\xspace}
\newcommand{\cH}{\ensuremath{{\mathcal H}}\xspace}
\newcommand{\cN}{\ensuremath{{\mathcal N}}\xspace}
\newcommand{\cM}{\ensuremath{{\mathcal M}}\xspace}

\newcommand{\cO}{\ensuremath{{\mathcal O}}\xspace}
\newcommand{\cI}{\ensuremath{{\mathcal I}}\xspace}
\newcommand{\cP}{\ensuremath{{\mathcal P}}\xspace}
\newcommand{\cR}{\ensuremath{{\mathcal R}}\xspace}
\newcommand{\cS}{\ensuremath{{\mathcal S}}\xspace}
\newcommand{\cU}{\ensuremath{{\mathcal U}}\xspace}
\newcommand{\cV}{\ensuremath{{\mathcal V}}\xspace}
\newcommand{\cPn}{\ensuremath{{\mathcal P}_n}\xspace}

\newcommand{\fA}{\ensuremath{\mathfrak{A}}\xspace}
\newcommand{\fB}{\ensuremath{\mathfrak{B}}\xspace}
\newcommand{\fC}{\ensuremath{\mathfrak{C}}\xspace}

\newcommand{\fa}{\ensuremath{\mathfrak{a}}\xspace}
\newcommand{\fb}{\ensuremath{\mathfrak{b}}\xspace}
\newcommand{\fc}{\ensuremath{\mathfrak{c}}\xspace}
\newcommand{\fd}{\ensuremath{\mathfrak{d}}\xspace}

\newcommand{\bA}{\ensuremath{{\bf A}}\xspace}
\newcommand{\bB}{\ensuremath{{\bf B}}\xspace}
\newcommand{\bK}{\ensuremath{{\bf K}}\xspace}
\newcommand{\bE}{\ensuremath{{\bf E}}\xspace}
\newcommand{\bN}{\ensuremath{{\bf N}}\xspace}
\newcommand{\bG}{\ensuremath{{\bf G}}\xspace}

\newcommand{\ba}{\ensuremath{{\bf a}}\xspace}
\newcommand{\bb}{\ensuremath{{\bf b}}\xspace}
\newcommand{\bc}{\ensuremath{{\bf c}}\xspace}

\newcommand{\bbE}{\ensuremath{\mathbb{E}}}
\newcommand{\bbR}{\ensuremath{\mathbb{R}}}
\newcommand{\bbQ}{\ensuremath{\mathbb{Q}}}
\newcommand{\bbP}{\ensuremath{\mathbb{P}}}
\newcommand{\bbZ}{\ensuremath{\mathbb{Z}}}
\newcommand{\bbN}{\ensuremath{\mathbb{N}}}
\newcommand{\bbNn}{\ensuremath{\mathbb{N}_0}}

\newcommand{\bbRnp}{\ensuremath{\bbR_{\geq 0}}}
\newcommand{\bbQnp}{\ensuremath{\bbQ_{\geq}}}
\newcommand{\bbZnp}{\ensuremath{\bbZ_{\geq}}}

\newcommand{\bbRsp}{\ensuremath{\bbR_>}}
\newcommand{\bbQsp}{\ensuremath{\bbQ_>}}
\newcommand{\bbZsp}{\ensuremath{\bbZ_>}}

\newcommand{\cp}{\textsf{P}\xspace}
\newcommand{\cnp}{\textsf{NP}\xspace}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\GN}{\mathbb{G}_n}

\newcommand{\rb}{\right\}\xspace}
\newcommand{\lb}{\left\{\xspace}
\newcommand{\lbr}{\left(\xspace} 
\newcommand{\rbr}{\right)\xspace}
\newcommand{\ndelta}{\ensuremath{\overline{\delta}}}

\newcommand{\oms}{\{\!\!\{}
\newcommand{\cms}{\}\!\!\}}

\newcommand{\trans}{^\mathsf{T}}
\renewcommand{\vec}[1]{\mathbf{#1}}

\usepackage[auth-lg]{authblk}
\newcommand{\cm}[1]{{{\textcolor{blue}{\textbf{[CM:} {#1}\textbf{]}}}}}
\newcommand{\ebk}[1]{{{\textcolor{green}{\textbf{[EBK:} {#1}\textbf{]}}}}}
\renewcommand*{\Affilfont}{\normalsize\normalfont}
\renewcommand*{\Authfont}{\sffamily}

\recalctypearea


\begin{document}

 
\begin{equation}\label{mwubound}
T = \left\lceil \frac{8 l \rho \ln(m)}{\varepsilon^2} \right\rceil 
\end{equation}
\begin{equation}\label{epsfeas}
\vec{A}_i \vec{\bar{x}} \geq b_i - \varepsilon,
\end{equation}

\begin{algorithm}[H]\mbox{\hfill}
\\\textbf{Input:} System of linear equations $\vec{A} \vec{x} \geq \vec{b}$, $\varepsilon > 0$,  stepsize $\eta > 0$, scaling constant $\rho$. \\
\textbf{Output:} $\vec{\bar{x}}$ satisfying~\cref{epsfeas} or \texttt{non-feasible}.
\begin{algorithmic}[1]
	\State Initialize constraint weights $\vec{w}_i \leftarrow 1$ for $i$ in $[m]$ 
	\State Initialize probabilities $\vec{p}_j \leftarrow \nicefrac{1}{m}$ for $i$ in $[m]$ 
	\State Set $T$ to according~\cref{mwubound}
	\For{$t \text{ in } [T]$}

	\State Call oracle: $\vec{x} \leftarrow \cO(\vec{p}\trans \vec{A} \vec{x} \geq \vec{p}\trans \vec{b})$, \textbf{if} $
	\vec{x} = \emptyset$ \Return  \texttt{non-feasible}
	\State Compute error signal $\vec{e}$, where
	\begin{equation*}
		\vec{e}_j \leftarrow \nicefrac{1}{\rho}\,(\vec{A}_j\trans \vec{x}(t) - \vec{b}_j)   
	\end{equation*}
	\State Perform gradient descent by $\vec{w}_i \leftarrow (1 - \eta \vec{e}_i) \vec{w}_i$ 
	\State Normalize weights $\vec{p}_j \leftarrow \nicefrac{\vec{w}_j}{\boldsymbol{\Gamma}(t)} $  for $i$ in $[m]$, where $\boldsymbol{\Gamma}(t) = \sum_{i \in [m]} \vec{w}_i$
	\State Update solution $\vec{\bar{x}} \leftarrow \vec{\bar{x}} + \vec{x}$
	\EndFor
	\State \Return Average over solution $ \nicefrac{\vec{\bar{x}}}{T} $
\end{algorithmic}
\caption{MWU for the LP feasibility problem.}
\label{alg:as}
\end{algorithm}
Same algorithm as above interpreted as a message passing algorithm:
\begin{algorithm}[H]\mbox{\hfill}
	\\\textbf{Input:} Biparite constraint graph $B(I)$, $\varepsilon > 0$,  stepsize $\eta > 0$, scaling constant $\rho$. \\
	\textbf{Output:} $\vec{\bar{x}}$ satisfying~\cref{epsfeas} or \texttt{non-feasible}.
	\begin{algorithmic}[1]
		\State Initialize weights $\vec{w}_j \leftarrow 1$ for each constraint node
		\State Initialize probabilities $\vec{p}_j \leftarrow \nicefrac{1}{m}$ for each constraint node
		\State Set $T$ to according~\cref{mwubound}
		\For{$t \text{ in } [T]$}
		\State For each constraint node $\vec{c}_j$ send $\vec{p}_j$ to adjacent node variable \Comment{Constraint to variable pass}
		\Statex
		\State Update each variable node $\vec{v}_i$ based on output $\vec{x}_i$ of oracle using $\vec{p}$
		\State Send $\vec{v}_i$ to each adjacent constraint, compute error signal $\vec{e}_i$ for each constraint $\vec{c}_j$\Comment{Variable to constraint pass}
		\begin{equation*}
		\vec{e}_j \leftarrow \nicefrac{1}{\rho}\,\Big(\sum_{i \in N(j)} \label{key}\vec{A}_{ji}  \vec{v}_i \Big) - \vec{b}_j   
		\end{equation*}
		\State Perform gradient descent by $\vec{w}_i \leftarrow (1 - \eta \vec{e}_i) \vec{w}_i$ 
		\State Normalize weights $\vec{p}_j \leftarrow \nicefrac{\vec{w}_j}{\boldsymbol{\Gamma}(t)} $  for $i$ in $[m]$, where $\boldsymbol{\Gamma}(t) = \sum_{i \in [m]} \vec{w}_i$
		\State Update solution $\vec{\bar{x}} \leftarrow \vec{\bar{x}} + \vec{x}$
		\EndFor
		\State \Return Average over solution $ \nicefrac{\vec{\bar{x}}}{T}$
	\end{algorithmic}
	\caption{MWU (Message passing version) for the LP feasibility problem.}
	\label{alg:as}
\end{algorithm}

Neuralized algorithm:
invariant, differentiable functions (e.g., NNs):
$f_{\text{V}}^{(0)}\colon \bbR^d \to \bbR^e$, $f_{\text{V}}^{(C)}\colon \bbR^d \to \bbR^e$, $f_{\text{X}}^{(0)}\colon \bbR^d \to \bbR^e$



\begin{algorithm}[H]\mbox{\hfill}
	\\\textbf{Input:}  Training set $\cS$, $\varepsilon > 0$, number of epochs $E$, number of rounds $T$, stepsize $\eta > 0$, scaling constant $\rho$.  \\
	\textbf{Output:} A trained model $f_{\boldsymbol{\theta}} \colon \cI \to [0,1]^n$.
	\begin{algorithmic}[1]
		\State Initialize $\vec{v}_i^{(0)}$, $\vec{c}_i^{(0)}$
		\State Initialize parameters $\boldsymbol{\theta} = (\boldsymbol{\theta}_{\text{V}}$, $\boldsymbol{\theta}_{\text{C}}$, $\boldsymbol{\theta}_{\text{X}})$
		\State Set weights $\vec{w}_j^{(0)} \leftarrow 1$, $\vec{p}_j^{(0)} \leftarrow \nicefrac{1}{m}$ for $j$ in $[m]$
		\For{$e \text{ in } [E]$}
		\State Sample $I = (\vec{c}, \vec{A}, \vec{b})$ with variable bias $\vec{\bar{b}}$ uniformly and at random from $\cS$, inducing the bipartite graph $B(I)$
		\For{$t \text{ in } [T]$}
		\State $\vec{v}_i^{(t)} \leftarrow {f}^{(t)}_{\text{V}} \hspace{-1pt} \big( \big\{ \hspace{-4.4pt} \big\{ \big( \vec{v}^{(t-1)}_i, \vec{c}^{(t-1)}_j, \vec{p}^{(t-1)}_j, \vec{A}_{ji} \big)\hspace{-1pt} \colon c_j \in N(v_i) \big\} \hspace{-4.4pt} \big\} \big)$ for all $i$ in $[n]$ 		
		\Comment{Constraint to variable pass}
		\State $\vec{x}_i^{(t)} \leftarrow {f}^{(t)}_{\text{X}} \hspace{-2pt} \left(\vec{v}_i^{(t-1)} \right)$ for all $i$ in $[n]$ \Comment{Generate variable assigment}
		\Statex
		\Statex
		\State $\vec{c}_j^{\hspace{1pt}(t)} \leftarrow {f}^{(t)}_{\text{C}} \hspace{-2pt} \left( \left\{ \hspace{-4.4pt} \left\{ \left( \vec{c}^{(t-1)}_j, \vec{v}^{(t)}_i, \vec{x}^{(t)}_i, \vec{A}_{ji}, \vec{b}_j \right)\hspace{-2pt} \colon i \in N(c_j) \right\} \hspace{-4.4pt} \right\} \right)$ for all $j$ in $[m]$ 		\Comment{Variable to constraint pass}
		\State $\vec{e}_j^{(t)} \, \leftarrow \nicefrac{1}{\rho}\,\Big(\sum_{i \in N(j)} \label{key}\vec{A}_{ji}  \vec{v}_i \Big) - \vec{b}_j$
		%\Statex
		\State $\vec{w}_j^{(t)} \leftarrow ( 1 -  \eta \vec{e}^{(t)}_j) \vec{w}_j^{(t-1)} $ 
		\State $\vec{p}_j \leftarrow \nicefrac{w_j^{(t)}}{\boldsymbol{\Gamma}(t)} $, where $\boldsymbol{\Gamma}(t) = \sum_{j \in [m]} \vec{w}^{(t)}_j$
		\State Update solution $\vec{\bar{x}} \leftarrow \vec{\bar{x}} + \vec{x}$
		\EndFor
		\State $\vec{\bar{x}} \leftarrow \nicefrac{1}{T} \sum_{t \in [T]} \vec{x}(t) $
		\State $L \leftarrow L + \ell(\vec{\bar{x}}, \vec{\bar b})$ 
		\State Gradient descent on $\nicefrac{1}{T} L$ with regard to $\boldsymbol{\theta}$
		\EndFor
		\State \Return $\boldsymbol{\theta}$
		
	\end{algorithmic}
	\caption{Data-driven MWU for variable bias prediction.}
	\label{alg:as2}
\end{algorithm}





\end{document}
